[TOC]

# 模型评价指标

准确率、精确率（查准率）、召回率（查全率）、F1值、ROC曲线的AUC值，都可以作为评价一个机器学习模型好坏的指标（evaluation metrics），而这些评价指标直接或间接都与混淆矩阵有关，前四者可以从混淆矩阵中直接计算得到，AUC值则要通过ROC曲线进行计算，而ROC曲线的横纵坐标又和混淆矩阵联系密切

---

## 混淆矩阵

对于一个二分类，可有如下混淆矩阵：

|                     |                |      真实类别       |    actual class     |
| :-----------------: | :------------: | :-----------------: | :-----------------: |
|                     |                |   Positive class    |   Negative class    |
|    **预测类别**     | Positive class | (True Positive)  TP | (False Positive) FP |
| **predicted class** | Negative class | (False Negative) FN | (True Negative) TN  |

行表示预测类别，列表示真实类别。在sklearn中，二分类问题下的混淆矩阵需要分别将表 中的predicted class和Actual class对调，将横纵坐标的positive class和negative class都分别对调，再重新计算混淆矩阵。

**TP、FP、TN、FN，第二个字母表示样本被预测的类别，第一个字母表示样本的预测类别与真实类别是否一致**

| **TP** |        真实类别为positive，模型预测的类别也为positive        |
| :----: | :----------------------------------------------------------: |
| **FP** | 预测为positive，但真实类别为negative，真实类别和预测类别不一致 |
| **FN** | 预测为negative，但真实类别为positive，真实类别和预测类别不一致 |
| **TN** |        真实类别为negative，模型预测的类别也为negative        |

---

## 准确率（accuracy）

$$
acc=\frac{TP+TN}{TP+TN+FP+FN}=\frac{TP+TN}{all data}
$$

准确率表示预测正确的样本（TP和TN）在所有样本（all data）中占的比例。在数据集不平衡时，准确率将不能很好地表示模型的性能。可能会存在准确率很高，而少数类样本全分错的情况，此时应选择其它模型评价指标。

---

## 精确率（查准率）和召回率（查全率）

**positive class 的精确率：**
$$
precision = \frac{TP}{TP + FP}=\frac{TP}{预测为positive的样本}
$$
**positive class的召回率：**
$$
recall= \frac{TP}{TP + FN}=\frac{TP}{真实为positive的样本}
$$
positive class的**精确率**表示在预测为positive的样本中真实类别为positive的样本所占比例；positive class的**召回率**表示在真实为positive的样本中模型成功预测出的样本所占比例。　

positive class的召回率只和真实为positive的样本相关，与真实为negative的样本无关；而精确率则受到两类样本的影响。

---

## $F_{1}$和$F_{\beta }$值

**$F_{1}$公式：**
$$
F_{1}=\frac{2}{\frac{1}{precision}+\frac{1}{recall}}=\frac{2*precision*recall}{precision+recall}
$$
$F_{1}$ 值就是精确率和召回率的调和平均值，$F_{1}$ 值认为精确率和召回率一样重要。

---

**$F_{\beta}$ 公式如下：**
$$
F_{\beta}=\frac{1+\beta^{2}}{\frac{1}{precision}+\frac{\beta^{2}}{recall}}=\frac{(1+\beta^{2})*precision*recall}{\beta^{2}precision+recall}
$$
在$\beta$=1时，$F_{\beta}$就是$F_{1}$值，此时$F_{\beta}$认为精确率和召回率一样重要；当β>1时，$F_{\beta}$认为召回率更重要；当0<β<1时，$F_{\beta}$认为精确率更重要。除了$F_{1}$值之外，常用的还有$F_{2}$和$F_{0.5}$

---

## ROC曲线和AUC值

AUC全称为Area Under Curve，表示一条曲线下面的面积，ROC曲线的AUC值可以用来对模型进行评价。ROC曲线如图 所示：

![](C:\Users\Administrator\Desktop\roc.png)

ROC曲线的纵坐标True Positive Rate（TPR）在数值上就等于positive class的recall，记作$recall_{positive} $值，横坐标False Positive Rate（FPR）在数值上等于(1 - negative class的recall)，记作(1 - $1-recall_{negative}$)如下所示：
$$
TPR = \frac{TP}{TP+FN}=recall_{positive}
$$

$$
FPR = \frac{FP}{FP+TN}=\frac{FP+TN-TN}{FP+TN}=1-\frac{TN}{FP+TN}=1-recall_{negative}
$$


通过对分类阈值θ（默认0.5）从大到小或者从小到大依次取值，我们可以得到很多组TPR和FPR的值，将其在图像中依次画出就可以得到一条ROC曲线，阈值θθ取值范围为[0,1]。

ROC曲线在图像上越接近左上角(0,1)模型越好，即ROC曲线下面与横轴和直线FPR = 1围成的面积（AUC值）越大越好。直观上理解，纵坐标TPR就是$recall_{positive} $值，横坐标FPR就是(1 - $recall_{positive} $)，前者越大越好，后者整体越小越好，在图像上表示就是曲线越接近左上角(0,1)坐标越好。

　　图 展示了３个模型的ROC曲线，要知道哪个模型更好，则需要计算每条曲线的AUC值，一般认为AUC值越大越好。AUC值由定义通过计算ROC曲线、横轴和直线FPR = 1三者围成的面积即可得到。