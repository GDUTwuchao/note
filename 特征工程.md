#  <center>特征工程

[TOC]

特征工程在机器学习流程中的位置：

![](C:\Users\Administrator\Desktop\特征工程\流程.png)

---

## 数值

要对数值型数据进行合理性检查，首先要看看它的量级。我们只需知道它是正的还是负的，还是只需在一个很粗的粒度上知道它的量级？这种合理性检查非常重要，尤其是对于那些自动产生的数值，比如计数（网站的每日访问量、餐馆的评价数量等）。

然后，还要考虑一下特征的尺度。它的最大值和最小值是多少？是否横跨多个数量级？如果模型是输入特征的平滑函数，那么它对输入的尺度是非常敏感的。例如，`3x + 1` 是输入 x 的一个简单线性函数，它的输出尺度直接取决于输入尺度。此外，k-均值聚类、最近邻方法、径向基核函数，以及所有使用欧氏距离的方法都属于这种情况。对于这类模型和模型成分，通常需要对特征进行标准化，以便将输出控制在期望的范围之内。

相反，逻辑函数对输入特征的尺度并不敏感。无论输入如何，这种函数的输出总是一个二值变量。例如，逻辑操作 AND 接受两个变量，当且仅当这两个输入都为真时，才输出 1。逻辑函数的另一个例子是阶梯函数（如：输入 x 是否大于 5 ？）。决策树模型中使用了输入特征的阶梯函数，因此，基于空间分割树的模型（决策树、梯度提升机、随机森林）对尺度是不敏感的。唯一的例外是，如果输入的尺度是随时间变化的，也就是说如果特征是某种累计值，那么它最终可能会超出训练树的取值范围。如果出现了这种情况，就必须定期地对输入尺度进行调整；另外一种解决方案是使用区间计数方法。

数值型特征的分布也非常重要，这种分布可以表示出一个特定值出现的概率。输入特征的分布对于某些模型来说比其他模型更重要。例如，线性回归模型的训练过程需要假定预测误差近似地服从高斯分布。这种假定通常是没有问题的，除了预测目标分布在多个数量级中的时候，在这种情况下，误差符合高斯分布的假定将不会被满足。一种解决方法是对输出目标进行转换，以消除数量级带来的影响。（严格说来，这应该称为目标工程，而不是特征工程。）对数变换（指数变换的一种特殊形式）可以使变量的分布更加接近于高斯分布。

### 标量，向量和空间

单独的数值型特征称为标量，标量的有序列表称为向量，向量位于向量空间中。在绝大多数机器学习应用
中，模型的输入通常表示为数值向量。

### 处理计数

在大数据时代，计数数据可以无限度地快速增长。用户可以无数次地重复播放一首歌曲或一部电影，也可以使用脚本反复地检查一个热门演出是否有余票，这都可以使播放计数和网站访问计数快速增长。当数据被大量且快速地生成时，很有可能包含一些极端值。这时就应该检查数据的尺度，确定是应该保留数据原始的数值形式，还是应该将它们转换成二值数据，或者进行粗粒度的分箱操作。为了说明这种思想，我们来看几个例子。

#### 二值化

将大于某个数值的设为1，小于的设为0

#### 区间量化(分箱)

- 固定宽度分箱

    通过固定宽度分箱，每个分箱中会包含一个具体范围内的数值。这些范围可以人工定制，也可以通过自动分段来生成，它们可以是线性的，也可以是指数性的。例如，我们可以按10 年为一段来将人员划分到多个年龄范围中：0~9 岁的在分箱 1 中、10~19 岁的在分箱 2中，等等。要将计数值映射到分箱，只需用计数值除以分箱的宽度，然后取整数部分。

    当数值横跨多个数量级时，最好按照 10 的幂（或任何常数的幂）来进行分组：`0~9，10 ~ 99, 100 ~ 999`等等。这时分箱宽度是呈指数增长的，从 `O(10)` 到 `O(100)`、`O(1000)` 以及更大。要将计数值映射到分箱，需要取计数值的对数。

    ```python
    import numpy as np
    
    #按照10进行等宽分箱
    small_count = np.random.randint(0,100,20)
    small_count
    # array([71, 56, 10, 82, 32, 75, 22,  3, 37, 59, 56, 75, 13, 29, 36, 11, 68,
    #        1, 99, 27])
    np.floor_divide(small_count,10)
    # array([7, 5, 1, 8, 3, 7, 2, 0, 3, 5, 5, 7, 1, 2, 3, 1, 6, 0, 9, 2],dtype=int32)
    
    #按照幂次方进行分箱
    large_counts = [296, 8286, 64011, 80, 3, 725, 867, 2215, 7689, 11495, 91897,44, 28, 7971, 926, 122, 22222]
    np.floor(np.log10(large_counts))
    #array([2., 3., 4., 1., 0., 2., 2., 3., 3., 4., 4., 1., 1., 3., 2., 2., 4.])
    ```

    

-  自适应分箱（分位数分箱）

    固定宽度分箱非常容易计算，但如果计数值中有比较大的缺口，就会产生很多没有任何数据的空箱子。根据数据的分布特点，进行自适应的箱体定位，就可以解决这个问题。这种方法可以使用数据分布的分位数来实现。

    分位数是可以将数据划分为相等的若干份数的值。例如，中位数（即二分位数）可以将数据划分为两半，其中一半数据点比中位数小，另一半数据点比中位数大。四分位数将数据四等分，十分位数将数据十等分，等等。

    使用`pd.qcut（）`进行分箱。`pd.quantie（[0.25, 0.5, 0.75]）`计算分位。

```python
pd.qcut(large_counts, 4
        #, labels=False
        #,labels = ['第一','第二','第三','第四']
       )
'''
[(122.0, 926.0], (926.0, 8286.0], (8286.0, 91897.0], (2.999, 122.0], (2.999, 122.0], ..., (2.999, 122.0], (926.0, 8286.0], (122.0, 926.0], (2.999, 122.0], (8286.0, 91897.0]]
Length: 17
Categories (4, interval[float64]): [(2.999, 122.0] < (122.0, 926.0] < (926.0, 8286.0] < (8286.0, 91897.0]]
'''
使用label的话输出为：
# array([1, 2, 3, 0, 0, 1, 1, 2, 2, 3, 3, 0, 0, 2, 1, 0, 3], dtype=int64)

#只能为serie的时候才能用
ser_large_counts = pd.Series(large_counts)
ser_large_counts.quantile([0.25,0.5,0.78])
```

----

### 对数变换

对数函数是指数函数的反函数，它的定义是 `log a (a x ) = x`，其中 `a` 是个正的常数，x 可以是任意正数。因为 `a 0 = 1`，所以有 `log a (1) = 0`。这意味着对数函数可以将 `(0, 1)` 这个小区间中的数映射到 `(-∞, 0)` 这个包括全部负数的大区间上。函数 `log 10 (x)` 可以将区间 `[1, 10]` 映射到`[0, 1]`，将 `[10, 100]` 映射到 `[1, 2]`，以此类推。换言之，对数函数可以对大数值的范围进行压缩，对小数值的范围进行扩展。`x` 越大，`log(x)` 增长得越慢。

![](C:\Users\Administrator\Desktop\特征工程\log.png)

**<center>对数函数压缩高值区间并扩展低值区间**

对于具有重尾分布的正数值的处理，对数变换是一个非常强大的工具。（与高斯分布相比，重尾分布的概率质量更多地位于尾部。）它压缩了分布高端的长尾，使之成为较短的尾部，并将低端扩展为更长的头部。

---

### 特征缩放/归一化

有些特征的值是有界限的，比如经度和纬度，但有些数值型特征可以无限制地增加，比如计数值。有些模型是输入的平滑函数，比如线性回归模型、逻辑回归模型或包含矩阵的模型，它们会受到输入尺度的影响。相反，那些基于树的模型则根本不在乎输入尺度有多大。如果模型对输入特征的尺度很敏感，就需要进行特征缩放。顾名思义，特征缩放会改变特征的尺度，有些人将其称为特征归一化。特征缩放通常对每个特征独立进行。下面讨论几种常用的特征缩放操作，每种操作都会产生一种不同的特征值分布。

#### min-max缩放

令 x 是一个独立的特征值（即某个数据点中的特征值），`min(x)` 和 `max(x)` 分别为这个特征在整个数据集中的最小值和最大值。`min-max` 缩放可以将所有特征值压缩（或扩展）到 `[0, 1]`区间中。`min-max` 缩放的公式如下：
$$
\frac{x-min(x)}{max(x)-min(x)}
$$


![](C:\Users\Administrator\Desktop\特征工程\minmax.png)

```python
from sklearn import preprocessing 
preprocessing.minmax_scale(data)
```

#### 特征标准化/方差缩放

它先减去特征的均值（对所有数据点），再除以方差，因此又称为方差缩放。缩放后的特征均值为 0，方差为 1。如果初始特征服从高斯分布，那么缩放后的特征也服从高斯分布。
$$
\widetilde{x}= \frac{x-\mu }{\sigma }
$$


![](C:\Users\Administrator\Desktop\特征工程\标准化.png)

```python
preprocessing.StandarScaler.fit_transform(data)
```

#### L2归一化

`L2`范数是坐标空间中向量长度的一种测量
$$
\widetilde{x}=\frac{x}{\left \| x \right \|_{2}}
$$
![](C:\Users\Administrator\Desktop\特征工程\l2.png)

![](C:\Users\Administrator\Desktop\特征工程\归一化.png)

```python
preprocessing.normalize(data)
```

----

### 交互特征

两个特征的**乘积**可以组成一对简单的交互特征，这种相乘关系可以用逻辑操作符 `AND` 来类比，它可以表示出由一对条件形成的结果：“该购买行为来自于邮政编码为 98121 的地区”`AND`“用户年龄在 18 和 35 岁之间”。这种特征在基于决策树的模型中极其常见，在广义线性模型中也经常使用。

```python
from sklearn import linear_model
from sklearn.model_selection import train_test_split
import sklearn.preprocessing as preproc

df.columns
Index(['url', 'timedelta', 'n_tokens_title', 'n_tokens_content',
'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens',
'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',
'average_token_length', 'num_keywords', 'data_channel_is_lifestyle',
'data_channel_is_entertainment', 'data_channel_is_bus',
'data_channel_is_socmed', 'data_channel_is_tech',
'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',
'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',
'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',
'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',
'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',
'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',
'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',
'global_sentiment_polarity', 'global_rate_positive_words',
'global_rate_negative_words', 'rate_positive_words',
'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',
'max_positive_polarity', 'avg_negative_polarity',
'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',
'title_sentiment_polarity', 'abs_title_subjectivity',
'abs_title_sentiment_polarity', 'shares'],
dtype='object')

# 选择与内容有关的特征作为模型的单一特征，忽略那些衍生特征
features = ['n_tokens_title', 'n_tokens_content',
... 'n_unique_tokens', 'n_non_stop_words', 'n_non_stop_unique_tokens',
... 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos',
... 'average_token_length', 'num_keywords', 'data_channel_is_lifestyle',
... 'data_channel_is_entertainment', 'data_channel_is_bus',
... 'data_channel_is_socmed', 'data_channel_is_tech',
... 'data_channel_is_world']

 X = df[features]
 y = df[['shares']]

# 创建交互特征对，跳过固定偏移项
X2 = preproc.PolynomialFeatures(include_bias=False).fit_transform(X)
X2.shape
(39644, 170)

# 为两个特征集创建训练集和测试集
X1_train, X1_test, X2_train, X2_test, y_train, y_test = \
train_test_split(X, X2, y, test_size=0.3, random_state=123)

>>> def evaluate_feature(X_train, X_test, y_train, y_test):
... """Fit a linear regression model on the training set and
... score on the test set"""
... model = linear_model.LinearRegression().fit(X_train, y_train)
... r_score = model.score(X_test, y_test)
... return (model, r_score)

# 在两个特征集上训练模型并比较R方分数
>>> (m1, r1) = evaluate_feature(X1_train, X1_test, y_train, y_test)
>>> (m2, r2) = evaluate_feature(X2_train, X2_test, y_train, y_test)
>>> print("R-squared score with singleton features: %0.5f" % r1)
>>> print("R-squared score with pairwise features: %0.10f" % r2)
R-squared score with singleton features: 0.00924
R-squared score with pairwise features: 0.0113276523
```

----

### 特征选择

特征选择技术可以精简掉无用的特征，以降低最终模型的复杂性，它的最终目的是得到一个简约模型，在不降低预测准确率或对预测准确率影响不大的情况下提高计算速度。为了得到这样的模型，有些特征选择技术需要训练不止一个待选模型。换言之，特征选择不是为了减少训练时间（实际上，一些技术会增加总体训练时间），而是为了减少模型评分时间。

粗略地说，特征选择技术可以分为以下三类。

- 过滤
    过滤技术对特征进行预处理，以除去那些不太可能对模型有用处的特征。例如，我们可以计算出每个特征与响应变量之间的**相关性**或**互信息**，然后过滤掉那些在某个阈值之下的特征。后面将讨论用于文本特征的这种技术。过滤技术的成本比下面描述的打包技术低廉得多，但它们没有考虑我们要使用的模型，因此，它们有可能无法为模型选择出正确的特征。我们最好谨慎地使用预过滤技术，以免在有用特征进入到模型训练阶段之前不经意地将其删除。
- 打包方法
    这些技术的成本非常高昂，但它们可以试验特征的各个子集，这意味着我们不会意外地删除那些本身不提供什么信息但和其他特征组合起来却非常有用的特征。打包方法将模型视为一个能对推荐的特征子集给出合理评分的黑盒子。它们使用另外一种方法迭代地对特征子集进行优化。
- 嵌入式方法
    这种方法将特征选择作为模型训练过程的一部分。例如，特征选择是决策树与生俱来的一种功能，因为它在每个训练阶段都要选择一个特征来对树进行分割。另一个例子是ℓ 1 正则项，它可以添加到任意线性模型的训练目标中。ℓ 1 正则项鼓励模型使用更少的特征，而不是更多的特征，所以又称为模型的稀疏性约束。嵌入式方法将特征选择整合为模型训练过程的一部分。它们不如打包方法强大，但成本也远不如打包方法那么高。与过滤技术相比，嵌入式方法可以选择出特别适合某种模型的特征。从这个意义上说，嵌入式方法在计算成本和结果质量之间实现了某种平衡。

----

## 文本数据：扁平化、过滤和分块

我们从**词袋**开始，这是**基于单词数量统计**的最简单的文本特征表示方法。与词袋密切相关的一种转换技术是 `tf-idf`，它本质上是一种特征缩放技术。

### 元素袋：将自然文本转化为扁平向量

对于文本数据，我们可以从一个单词数量的统计列表开始，这称为词袋`（bag-of-words，BoW）`。这个单词数量列表并不试图找出有意义的实体，比如 `Emma` 或 `raven（乌鸦）`。但这两个单词在我们的示例文本中被提及了多次，它们出现的次数远高于那些随机出现的单词，比如`“hello”`。对于像文档分类这样的简单任务来说，单词数量统计通常就够用了。这种技术还可以用于信息提取，它的目标是提取出一组与查询文本相关的文档。这两种任务都可以凭借单词级别的特征圆满地完成，因为特定词是否存在于文档中这个指标可以很好地标识文档的主题内容。

#### 词袋

在词袋特征化中，一篇文本文档被转化为一个计数向量。（向量就是 n 个数值的集合。）这个计数向量包含词汇表中所有可能出现的单词。如果某个单词（比如`“aardvark”`）在文档中出现了 3 次，那么特征向量在对应于这个单词的位置就有一个计数值 3。如果词汇表中的某个单词没有出现在文档中，那么它的计数值就是 0。例如，文本`“it is a puppy and it isextremely cute”`具有图 中的词袋表示。

![](C:\Users\Administrator\Desktop\特征工程\词袋.png)































### 使用过滤获取清洁特征

### 意义的单位：从单词，n元词到短语











































































